{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bbtGmH0XfQWzVKROhiIP8x5EAv6XuohJ","timestamp":1671662700635},{"file_id":"1j8WsT-QGlXRfUUf5LwE9jhQ-0wIBL8iO","timestamp":1668252941319}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["Original description from andite's Colab:\n","\n","> Cafe's TI repo Colab implemented by **andite**.\n","> \n","> Textual Inversion repo from **Cafe**. Thank you for this wonderful TI repo, all credits goes to him. :)\n","> \n","> https://github.com/cafeai/stable-textual-inversion-cafe\n","> \n","> <fieldset>\n","> Discord:\n","> \n","> Cafe - `Starport — かふぇ#0438`\n","> \n","> andite - `andite#8484`\n","> </fieldset>\n","> \n","> Please contact me through discord if you found any errors in this repo or colab."],"metadata":{"id":"TXAVZ_66-D0H"}},{"cell_type":"markdown","source":["Colab for Cafe's TI repo, slight refactor by Jamm. [Thanks to **andite** for the original Colab](https://colab.research.google.com/drive/1bbtGmH0XfQWzVKROhiIP8x5EAv6XuohJ?usp=sharing). \\[Version 12/8/22\\]\n","\n","<font size=\"+2\">How to use this Colab</font>\n","\n","1. Prepare your image dataset and upload it to a folder in Google Drive\n","2. Follow the instructions in this Colab"],"metadata":{"id":"rRW4mTfaqQeG"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"S8m4ylgx3vS2","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671812604960,"user_tz":0,"elapsed":513,"user":{"displayName":"Wong Chun Ho","userId":"02298666774353839854"}},"outputId":"0539070b-b3af-4356-bb1f-a704125cebda"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001bGPU Connected! Currently using Tesla T4\n","You may continue.\n"]}],"source":["#@title Test your GPU runtime\n","from subprocess import getoutput\n","\n","output = getoutput('nvidia-smi --query-gpu=gpu_name --format=csv')\n","if \"name\" in output:\n","  print('\u001bGPU Connected! Currently using', output[5:]) \n","  print('You may continue.')\n","else:\n","  raise RuntimeError(\"No GPU accelerator is connected. Please connect to a GPU Runtime.\")\n"]},{"cell_type":"code","source":["#@title Mount a remote to `/content/drive/MyDrive`\n","#@markdown You can mount cloud storage using **Google Drive**, or any other cloud service through **rclone**.\n","#@markdown\n","#@markdown Mount backend:\n","\n","mount_backend = \"drive\" #@param [\"drive\", \"rclone\"]\n","\n","#@markdown ----------\n","#@markdown ### rclone settings (ignore these if you're using Google Drive)\n","#@markdown\n","#@markdown If using rclone, open the Files tab and **upload your configuration to `/content/rclone.conf` before running this cell.**\n","\n","#@markdown The remote path to be mounted here, as defined in your rclone config (e.g. `\"gdrive:/\"`)\n","remote_path = \"gdrive:\" #@param {type:\"string\"}\n","\n","from pathlib import Path\n","import subprocess\n","import shutil\n","from google.colab import drive\n","\n","def _rclone_mount(remote_path):\n","  global _rclone_is_mounted\n","  Path(\"/content/drive/MyDrive\").mkdir(parents=True, exist_ok=True)\n","  !rclone mount --config $CONFIG_PATH $remote_path /content/drive/MyDrive --vfs-cache-mode full --daemon\n","  _rclone_is_mounted = True\n","\n","def _rclone_unmount():\n","  global _rclone_is_mounted\n","  !fusermount -u /content/drive/MyDrive\n","  !rmdir /content/drive/MyDrive\n","  _rclone_is_mounted = False\n","\n","if '_drive_is_mounted' not in globals():\n","  _drive_is_mounted = False\n","if '_rclone_is_mounted' not in globals():\n","  _rclone_is_mounted = False\n","\n","if _rclone_is_mounted:\n","  _rclone_unmount()\n","  _rclone_is_mounted = False\n","if _drive_is_mounted:\n","  drive.flush_and_unmount()\n","  _drive_is_mounted = False\n","\n","if mount_backend == \"drive\":\n","  drive.mount('/content/drive')\n","  _drive_is_mounted = True\n","else:\n","  CONFIG_PATH = Path(\"/content/rclone.conf\")\n","  if not CONFIG_PATH.exists():\n","    raise RuntimeError(\"Please create your config first, at: /content/rclone.conf\")\n","\n","  # Install rclone if not yet installed\n","  if not shutil.which(\"rclone\"):\n","    # https://rclone.org/install/\n","    !sudo -v ; curl https://rclone.org/install.sh | sudo bash\n","\n","  _rclone_mount(remote_path)\n","\n","  from IPython.display import clear_output\n","  clear_output()\n","  print(f\"Mounted successfully: {remote_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"9w7UyeET6S8A","executionInfo":{"status":"ok","timestamp":1671812627738,"user_tz":0,"elapsed":5326,"user":{"displayName":"Wong Chun Ho","userId":"02298666774353839854"}},"outputId":"ebada85d-45ba-41f7-8127-27183e89a260"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted successfully: gdrive:\n"]}]},{"cell_type":"code","source":["#@title Download models for training\n","\n","#@markdown Use this cell to download models to /content/models. You can run this cell multiple times to download multiple models if needed.\n","#@markdown\n","#@markdown **Note:** AnythingV3 can't be used for training for some reason, trying to use it produces garbage output.\n","#@markdown\n","#@markdown The URL of the model to be downloaded:\n","#@markdown - This supports Google Drive, Huggingface, and direct download links\n","ModelUrl = \"https://huggingface.co/andite/models/resolve/main/nai-wd.ckpt\" #@param [\"https://huggingface.co/andite/models/resolve/main/nai-wd.ckpt\"] {allow-input:true}\n","\n","# ===== Setup downloader =====\n","\n","import shutil\n","import shlex\n","\n","def _dl_install_prerequisites():\n","  # install gdown\n","  !pip install -q -U gdown\n","\n","  # Install aria2 if not yet installed\n","  if not shutil.which(\"aria2c\"):\n","    !apt -qq install -y aria2\n","\n","#@markdown HuggingFace Token (Optional)\n","#@markdown - Will use a default token if not provided\n","HuggingFaceToken = \"\" #@param {type:\"string\"}\n","HF_TOKEN = HuggingFaceToken.strip() or \"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\"\n","\n","def _dl_hugging_face(url, output_path=None):\n","  hf_header = f\"Authorization: Bearer {HF_TOKEN}\"\n","  if output_path is not None and output_path.is_dir():\n","    output_args = f\"-d {shlex.quote(str(output_path))}\"\n","  elif output_path is not None:\n","    output_args = f\"-o {shlex.quote(str(output_path))}\"\n","  else:\n","    output_args = \"\"\n","  !aria2c --summary-interval=10 -c --header={shlex.quote(hf_header)} -x 10 -k 1M -s 10 {output_args} {shlex.quote(url)}\n","\n","def _dl_gdrive(url, output_path=None):\n","  args = f\"{shlex.quote(url)} --continue --fuzzy\"\n","  if output_path is not None:\n","    args += f\" -O {shlex.quote(str(output_path))}\"\n","  if '/folders/' in url:\n","    args += \" --folder\"\n","  !gdown $args\n","\n","def _dl_generic(url, output_path=None):\n","  if output_path is not None and output_path.is_dir():\n","    output_args = f\"-d {shlex.quote(str(output_path))}\"\n","  elif output_path is not None:\n","    output_args = f\"-o {shlex.quote(str(output_path))}\"\n","  else:\n","    output_args = \"\"\n","  !aria2c --summary-interval=10 --seed-ratio=0.1 --allow-overwrite=true {output_args} {shlex.quote(url)}\n","\n","def download(url, output_path=None):\n","  output_path = Path(output_path)\n","  if 'huggingface.co' in url:\n","    _dl_hugging_face(url, output_path)\n","  elif 'drive.google.com' in url:\n","    _dl_gdrive(url, output_path)\n","  else:\n","    _dl_generic(url, output_path)\n","\n","_dl_install_prerequisites()\n","\n","# ====================\n","\n","from pathlib import Path\n","\n","MODEL_DIR = Path(\"/content/models\")\n","MODEL_DIR.mkdir(exist_ok=True)\n","\n","download(ModelUrl, output_path=MODEL_DIR)\n","\n","from IPython.display import clear_output\n","clear_output()\n","\n","import os\n","print(f\"List of models found in {MODEL_DIR}:\")\n","model_entries = list(x for x in os.scandir(MODEL_DIR) if x.name.lower().endswith(\".ckpt\") or x.name.lower().endswith(\".safetensors\"))\n","if len(model_entries) == 0:\n","  print(f\"  (No models found)\")\n","else:\n","  for entry in model_entries:\n","    print(f\"  {entry.path}\")\n","    if entry.name.lower().endswith(\".safetensors\"):\n","      print(\"    WARNING: Safetensors are NOT supported, trying to load them will cause an error!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"Tzcekrno7r1c","executionInfo":{"status":"ok","timestamp":1671812660033,"user_tz":0,"elapsed":30733,"user":{"displayName":"Wong Chun Ho","userId":"02298666774353839854"}},"outputId":"a927d018-e7c1-4727-de17-57566b5b229b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["List of models found in /content/models:\n","  /content/models/nai-wd.ckpt\n"]}]},{"cell_type":"code","source":["#@title Get the repo for textual inversion.\n","#@markdown This will clone Cafe's TI repo to this Colab's storage. This cell must always be executed in a new session.\n","\n","from pathlib import Path\n","\n","repo_path = Path(\"/content/stable-textual-inversion-cafe\")\n","!git clone https://github.com/cafeai/stable-textual-inversion-cafe.git $repo_path\n","\n","# generate configuration files\n","def fetch_text(url: str):\n","  import urllib.request\n","  with urllib.request.urlopen(url) as response:\n","    return response.read().decode()\n","\n","with open(\"/content/artstyle.yaml\", \"w\", encoding=\"utf8\") as f:\n","  f.write(fetch_text(\"https://gist.githubusercontent.com/jamesWalker55/41fd710256773ad3ca4b270d841cdad0/raw/artstyle.yaml\"))\n","\n","with open(\"/content/character.yaml\", \"w\", encoding=\"utf8\") as f:\n","  f.write(fetch_text(\"https://gist.githubusercontent.com/jamesWalker55/41fd710256773ad3ca4b270d841cdad0/raw/character.yaml\"))\n","\n","from IPython.display import clear_output\n","clear_output()\n","print(\"Done.\")"],"metadata":{"cellView":"form","id":"MDttzbd74sh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671812660850,"user_tz":0,"elapsed":821,"user":{"displayName":"Wong Chun Ho","userId":"02298666774353839854"}},"outputId":"13c43e82-ed29-40a5-e4e7-a2fe84984e2c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Done.\n"]}]},{"cell_type":"code","source":["#@title Install python packages. \n","#@markdown *It is highly required to install the packages or else the training won't work.*\n","#@markdown\n","#@markdown **After installation, the runtime will crash.** This is expected and how it's supposed to be, just proceed to the next cell after it restarts.\n","\n","%cd $repo_path\n","!pip install omegaconf einops pytorch-lightning==1.6.5 test-tube transformers kornia -e git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers -e git+https://github.com/openai/CLIP.git@main#egg=clip\n","!pip install setuptools==59.5.0\n","!pip install pillow==9.0.1\n","!pip install torchmetrics==0.6.0\n","!pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n","!pip install torchtext==0.13.1\n","!pip install -e .\n","\n","from IPython.display import clear_output\n","clear_output()\n","\n","print(\"Done.\")\n","print(\"The runtime is now going to crash - this is normal!\")\n","\n","import os\n","import time\n","time.sleep(1)\n","os._exit(00)\n"],"metadata":{"id":"rDCcZcop4pUa","cellView":"form","outputId":"39270b9c-6526-494c-e4d7-55e3e318cf34","colab":{"base_uri":"https://localhost:8080/","height":1000}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Done.\n","The runtime is now going to crash - this is normal!\n"]}]},{"cell_type":"markdown","source":["## Training\n","\n","Use the cells below to train an embedding. Run the **Configuration** cell first, then:\n","\n","- If you haven't trained an embedding yet, use the **Start training a new TI** cell.\n","- If you're resuming training from a previous session, use the **Resume training an existing TI** cell.\n","\n","Note: The cells will train forever. If you're satisfied with the results already, just stop the cell from running and it will still save the current embedding.\n","\n","> <font color='red' size=\"+2\">**DO NOT OVERTRAIN.**</font>\n",">\n","> The amount of steps you need depends on what you are training, and also the amount of datasets you have. 10k steps and below are usually fine for this repo, but you also have to see for yourself by testing your embeds. Do lots of comparisons with grids.\n",">\n","> For characters, you only need around 3-5k steps depending on its complexity. \n",">\n","> However, you'll need a lot more for artstyles. I can't give an estimate since it really depends on what kind of artstyle you're trying to train. The more complex it is, the more steps you need. Lowering the learning rate is also recommended so it wouldn't skip some details and have a less chance of loss rate.\n"],"metadata":{"id":"IQuNBMLgg6Gc"}},{"cell_type":"code","source":["#@title Configuration **(Required)**\n","\n","import os\n","import re\n","import shlex\n","from pathlib import Path\n","\n","#@markdown Path to store your training projects (and training images):\n","#@markdown - **Training preview images and checkpoints/pt files will be stored here**\n","#@markdown - Note: The actual training preview images are named `samples_scaled_gs` from the logs\n","#@markdown   - The `inputs_gs`, `reconstruction_gs`, `samples_gs` files aren't actual training preview images - ignore them.\n","log_path = '/content/drive/MyDrive/textual-inversion-cafe/projects' #@param {type:\"string\"}\n","# create the folder in case it doesn't exist\n","!mkdir -p {shlex.quote(log_path)}\n","\n","#@markdown The model to be used for training:\n","model_path = \"/content/models/nai-wd.ckpt\" #@param [\"/content/models/nai-wd.ckpt\"] {allow-input:true}\n","\n","repo_path = Path(\"/content/stable-textual-inversion-cafe\")\n","\n","PROJECT_NAME_REGEX = r\"(.+)(\\d\\d\\d\\d-\\d\\d-\\d\\dT\\d\\d-\\d\\d-\\d\\d)_(.+)\"\n","\n","def get_projects(log_path):\n","  \"\"\"Get information about projects in a directory, sorted by newest date (i.e. newer projects come first)\"\"\"\n","  projects = []\n","  for entry in os.scandir(log_path):\n","    if not entry.is_dir():\n","      continue\n","    x = re.fullmatch(PROJECT_NAME_REGEX, entry.name)\n","    if x is None:\n","      print(f\"WARNING: Failed to parse project info for path: {entry.path}\")\n","      continue\n","    _, date, name = x.groups()\n","    info = {'path': Path(entry.path), 'name': name, 'date': date}\n","    projects.append(info)\n","  projects.sort(key=lambda x: x['date'], reverse=True)\n","  return projects\n","\n","def get_latest_project(log_path, project_name):\n","  # This assumes that the list of projects is in reverse chronological order\n","  for proj in get_projects(log_path):\n","    if proj['name'] == project_name:\n","      return proj\n","  raise RuntimeError(f\"Failed to find project {project_name.__repr__()} in path: {log_path}\")\n","\n","def get_project(log_path, project_name, date):\n","  for proj in get_projects(log_path):\n","    if proj['name'] == project_name and proj['date'] == date:\n","      return proj\n","  raise RuntimeError(f\"Failed to find project {project_name.__repr__()} @ {date} in path: {log_path}\")\n","\n","def get_embed_no(name):\n","  x = re.search(r'(\\d+)\\.pt', name)\n","  if x is None:\n","    return None\n","  return int(x.group(1))\n","\n","def get_project_paths(proj):\n","  project_name = proj['name']\n","  last_checkpoint = proj['path'] / \"checkpoints/last.ckpt\"\n","  if not last_checkpoint.exists():\n","    raise FileNotFoundError(f\"Failed to find the last checkpoint for project {project_name.__repr__()}: {last_checkpoint}\")\n","  config_paths = [x.path for x in os.scandir(proj['path'] / 'configs') if x.name.endswith('.yaml')]\n","  if len(config_paths) == 0:\n","    raise FileNotFoundError(f\"Failed to find any configuration for project {project_name.__repr__()} in: {proj['path'] / 'configs'}\")\n","  # The 'embeddings.pt' file should be exactly the same as the latest embedding, e.g. 'embeddings_gs-3600.pt'\n","  last_embed = proj['path'] / \"checkpoints/embeddings.pt\"\n","  if not last_embed.exists():\n","    embeds = [x for x in os.scandir(proj['path'] / \"checkpoints\") if x.name.lower().endswith(\".pt\")]\n","    max_embed_steps = max(get_embed_no(x.name) or 0 for x in embeds)\n","    last_embed = proj['path'] / f\"checkpoints/embeddings_gs-{max_embed_steps}.pt\"\n","    if not last_embed.exists():\n","      raise FileNotFoundError(f\"Failed to find the last embed for project {project_name.__repr__()}: {last_checkpoint}\")\n","  return config_paths, last_checkpoint, last_embed\n","\n","def temp():\n","  projects = get_projects(log_path)\n","  if len(projects) == 0:\n","    print(\"No existing projects found in log_path\")\n","    return\n","  print(\"List of existing projects found in log_path:\")\n","  for proj in projects:\n","    print(f\"  {proj['date']} - {proj['name']}\")\n","\n","temp()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","id":"3JG9wyyBAurJ","executionInfo":{"status":"ok","timestamp":1671812977389,"user_tz":0,"elapsed":1134,"user":{"displayName":"Wong Chun Ho","userId":"02298666774353839854"}},"outputId":"dad592bc-9795-4509-9af3-64c39a4efbc9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: -c: line 0: syntax error near unexpected token `('\n","/bin/bash: -c: line 0: `mkdir -p {shlex.quote(log_path)}'\n","List of existing projects found in log_path:\n","  2022-12-22T22-00-19 - mayogiiStyleTI3\n","  2022-12-22T19-58-03 - mayogiiStyleTI3\n","  2022-12-22T18-24-04 - mayogiiStyleTI2\n","  2022-12-21T11-56-18 - mayogiiStyleTI\n","  2022-12-21T03-32-24 - mayogiiStyleTI\n","  2022-12-19T21-34-06 - tomokoTI2\n","  2022-12-19T21-28-23 - tomokoTI\n","  2022-12-19T16-54-51 - tomokoTI2\n","  2022-12-19T04-44-38 - tomokoTI\n","  2022-12-06T13-23-19 - shioriTI\n"]}]},{"cell_type":"code","source":["#@title Start training a new TI\n","\n","#@markdown Choose the name of the project:\n","project_name = \"my-cool-project\" #@param {type:\"string\"}\n","\n","#@markdown The folder of images you will use for training:\n","#@markdown - Assuming that your images have been uploaded to Google Drive, prefix the location of your images with `/content/drive/MyDrive` _(e.g. `/content/drive/MyDrive/textual-inversion-cafe/images/my-cool-dataset`)_\n","img_dir = \"/content/drive/MyDrive/textual-inversion-cafe/images/cool-images\" #@param {type:\"string\"}\n","\n","#@markdown Set initializer_word to `illustration` for artstyle, or `character` for character.\n","#@markdown - Warning, initializer_word isn't supposed to be the file name for your embed.\n","#@markdown - You can use a custom word though if you'd like.\n","initializer_word = \"character\" #@param ['illustration', 'character'] {allow-input: true}\n","\n","#@markdown The training configuration file. You can click the links below to start editing the config files:\n","#@markdown > artstyle: /content/artstyle.yaml\n","#@markdown >\n","#@markdown > character: /content/character.yaml\n","#@markdown\n","#@markdown In line 29, set your vector tokens depending on the amount of training images you have.\n","#@markdown\n","#@markdown Number of Images|Number of Tokens\n","#@markdown ---|---\n","#@markdown 45+ images|8 tokens\n","#@markdown 100-150+ images|16 tokens\n","#@markdown 300++ images|20 or more tokens\n","#@markdown\n","#@markdown _(Optional)_ In line 27, you can add some additional initializer words inside the square brackets if you want the training to focus more or make a certain part more accurate. _(I am not talking about the initializer_word field in this cell)_\n","#@markdown\n","#@markdown > For example, put `\"hair\"` inside the square brackets if you want it to focus more on the accuracy for the hair especially if it's complex or unique. You could also put words like `\"painting\"` or `\"lighting\"`.\n","#@markdown\n","#@markdown _(Optional)_ In line 2, you can set a custom learning rate, but `5.0e-03` / `0.005` is the default and the safest value.\n","\n","config_path = \"/content/character.yaml\" #@param [\"/content/artstyle.yaml\", \"/content/character.yaml\"] {allow-input:true}\n","\n","import shlex\n","\n","def train_new(config_path, model_path, project_name, img_dir, init_word):\n","  %cd $repo_path\n","  !python \"main.py\" \\\n","   -t --no-test \\\n","   --gpus 1 \\\n","   --base {shlex.quote(config_path)} \\\n","   --actual_resume {shlex.quote(model_path)} \\\n","   -n {shlex.quote(project_name)} \\\n","   --data_root {shlex.quote(img_dir)} \\\n","   --init_word {shlex.quote(init_word)} \\\n","   --logdir {shlex.quote(log_path)}\n","\n","train_new(config_path, model_path, project_name, img_dir, initializer_word)\n"],"metadata":{"cellView":"form","id":"iuZ0Gw1sIciE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Resume training an existing TI\n","\n","#@markdown Enter the project you want to resume training from:\n","#@markdown - When you ran the \"Configuration\" cell, it printed out a list of project names and dates\n","project_name = \"my-cool-project\" #@param {type:\"string\"}\n","\n","#@markdown **(Optional)** Use a specific date/version of the project for training. If left empty, use the latest version. (e.g. `2022-12-19T21-34-06`)\n","project_date = \"\" #@param {type:\"string\"}\n","\n","#@markdown The folder of images you used for training:\n","img_dir = \"/content/drive/MyDrive/textual-inversion-cafe/images/cool-images\" #@param {type:\"string\"}\n","\n","#@markdown Set initializer_word to `illustration` for artstyle, or `character` for character.\n","#@markdown - Warning, initializer_word isn't supposed to be the file name for your embed.\n","#@markdown - You can use a custom word though if you'd like.\n","initializer_word = \"character\" #@param ['illustration', 'character'] {allow-input: true}\n","\n","import shlex\n","\n","def train_resume(config_paths, model_path, project_name, img_dir, init_word, last_embed, last_checkpoint):\n","  %cd $repo_path\n","  config_args = [shlex.quote(x) for x in config_paths]\n","  config_args = \" \".join(config_args)\n","  !python \"main.py\" \\\n","   -t --no-test \\\n","   --gpus 1 \\\n","   --base {config_args} \\\n","   --actual_resume {shlex.quote(model_path)} \\\n","   -n {shlex.quote(project_name)} \\\n","   --data_root {shlex.quote(img_dir)} \\\n","   --init_word {shlex.quote(init_word)} \\\n","   --embedding_manager_ckpt {shlex.quote(str(last_embed))} \\\n","   --resume_from_checkpoint {shlex.quote(str(last_checkpoint))} \\\n","   --logdir {shlex.quote(log_path)}\n","\n","def temp():\n","  proj = get_latest_project(log_path, project_name)\n","  config_paths, last_checkpoint, last_embed = get_project_paths(proj)\n","  train_resume(config_paths, model_path, project_name, img_dir, initializer_word, last_embed, last_checkpoint)\n","\n","temp()\n"],"metadata":{"cellView":"form","id":"LDI_RceoIZjo"},"execution_count":null,"outputs":[]}]}
